{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "diverse-parcel",
   "metadata": {},
   "source": [
    "# dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "talented-official",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import fasttext as fastText\n",
    "from wordfreq import word_frequency, zipf_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "combined-beginning",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cardiac-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext():\n",
    "    #load pretrained fasttext crawl-300d-2M-subword.zip\n",
    "    print('loading word embeddings...')\n",
    "    embeddings_index = {}\n",
    "    f = open('/sandata/lexica_creation/crawl-300d-2M.vec',encoding='utf-8')\n",
    "    for idx, line in enumerate(f):\n",
    "        if idx != 0:\n",
    "            values = line.strip().rsplit(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('found %s word vectors' % len(embeddings_index))\n",
    "\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "familiar-institution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb_ls(ls):\n",
    "    # input: list of seed words\n",
    "    # output: list of corresponding embeddings\n",
    "    ls_seed_emb = []\n",
    "    for seed in ls:\n",
    "        seed_emb = np.mean([fasttext[x] for x in seed.split(' ') if x in fasttext], axis = 0)\n",
    "        ls_seed_emb.append(seed_emb)\n",
    "    return ls_seed_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nominated-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_emb(ls_emb):\n",
    "    # input: list of embeddings\n",
    "    # output: center embedding\n",
    "    return np.mean(ls_emb, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "increased-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_words(seed_center_emb,output_size):\n",
    "    # input: center embedding\n",
    "    # output: list of all words with cosine similarity to the center, descending order\n",
    "    final_list = []\n",
    "    min_sim = 0\n",
    "    \n",
    "    all_words = list(fasttext.keys())\n",
    "    for word in tqdm(all_words):\n",
    "        cos_sim = cosine_similarity([seed_center_emb],[fasttext[word]])[0][0]\n",
    "        if cos_sim > min_sim:\n",
    "            if len(final_list) == output_size:\n",
    "                final_list = final_list[:-1]\n",
    "            final_list.append((word,cos_sim))\n",
    "            final_list.sort(key = lambda x: x[1], reverse = True)\n",
    "            min_sim = final_list[-1][1]\n",
    "    \n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "parallel-height",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexica_creation(seed_words, output_size):\n",
    "    ls_seed_emb = get_emb_ls(seed_words)\n",
    "    seed_center_emb = get_center_emb(ls_seed_emb)\n",
    "    lexica = sort_words(seed_center_emb,output_size)\n",
    "    return lexica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "supposed-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_seed_words(file):\n",
    "    seed_words = []\n",
    "    with open(file,'r') as file:\n",
    "        for word in file:      \n",
    "            seed_words.append(word.lower().strip()) \n",
    "    return seed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-looking",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-payday",
   "metadata": {},
   "source": [
    "### input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fiscal-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_words_indv = load_seed_words(\"individualist_seed.txt\")\n",
    "seed_words_coll = load_seed_words(\"collectivist_seed.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-suspension",
   "metadata": {},
   "source": [
    "### initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "accomplished-tissue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading word embeddings...\n",
      "found 1999995 word vectors\n"
     ]
    }
   ],
   "source": [
    "fasttext = load_fasttext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4e4144e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "vectors = fastText.load_model('/sandata/lexica_creation/wiki.en.bin')\n",
    "wiki_en_words, freqs = vectors.get_words(include_freq=True)\n",
    "wiki_en_freqs = dict(zip(wiki_en_words, freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2398cdc7",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "received-stationery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get nearest words above a certain distance threshold\n",
    "def get_nearest_words(word, threshold_wiki, threshold_wordfreq):\n",
    "    nearest_words = lexica_creation([word], 100)\n",
    "    final_words = []\n",
    "    for n in nearest_words:\n",
    "        if(n[1] > threshold):\n",
    "            final_words.append(n)\n",
    "    return final_words\n",
    "\n",
    "def save_dict_to_file(dic, filename):\n",
    "    f = open(filename,'w')\n",
    "    f.write(str(dic))\n",
    "    f.close()\n",
    "\n",
    "def load_dict_from_file(filename):\n",
    "    f = open(filename,'r')\n",
    "    data=f.read()\n",
    "    f.close()\n",
    "    return eval(data)\n",
    "\n",
    "def parse_list(lexica, threshold_wiki, threshold_wordfreq):\n",
    "    final_lexica = []\n",
    "    for w in lexica:\n",
    "        word = w[0]#.replace(\"-\", \" \")\n",
    "        try:\n",
    "            freq_wiki = wiki_en_freqs[word]\n",
    "        except:\n",
    "            freq_wiki = 0\n",
    "        freq_wordfreq = word_frequency(word, 'en', wordlist='best', minimum=0.0)\n",
    "        if(freq_wiki > threshold_wiki or freq_wordfreq > threshold_wordfreq):\n",
    "            final_lexica.append(w)\n",
    "    return final_lexica\n",
    "\n",
    "def print_excel(lexica):\n",
    "    for w in lexica:\n",
    "        print(w[0] + \", \" + str(w[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bulgarian-sector",
   "metadata": {},
   "source": [
    "### lexica creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "unnecessary-capital",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "threshold = 0.6\n",
    "SAVED = True\n",
    "\n",
    "if(SAVED):\n",
    "    nearest_words_indv = load_dict_from_file(\"nearest_words_indv.txt\")\n",
    "    nearest_words_coll = load_dict_from_file(\"nearest_words_coll.txt\")\n",
    "    lexica_indv = load_dict_from_file(\"lexica_indv.txt\")\n",
    "    lexica_coll = load_dict_from_file(\"lexica_coll.txt\")\n",
    "else:\n",
    "    #get nearest individualist words\n",
    "    nearest_words_indv = {}\n",
    "    for w in seed_words_indv:\n",
    "        nearest_words = get_nearest_words(w, threshold)\n",
    "        nearest_words_indv[w] = nearest_words\n",
    "\n",
    "    #get nearest collectivist words\n",
    "    nearest_words_coll = {}\n",
    "    for w in seed_words_coll:\n",
    "        nearest_words = get_nearest_words(w, threshold)\n",
    "        nearest_words_coll[w] = nearest_words\n",
    "\n",
    "    #get full lexica\n",
    "    lexica_indv = lexica_creation(seed_words_indv,1000)\n",
    "    lexica_coll = lexica_creation(seed_words_coll,1000)\n",
    "    \n",
    "    #save outputs\n",
    "    save_dict_to_file(nearest_words_indv, \"nearest_words_indv.txt\")\n",
    "    save_dict_to_file(nearest_words_coll, \"nearest_words_coll.txt\")\n",
    "    save_dict_to_file(lexica_indv, \"lexica_indv.txt\")\n",
    "    save_dict_to_file(lexica_coll, \"lexica_coll.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d18b08",
   "metadata": {},
   "source": [
    "### Overlap large cluster lexicon with individual cluster lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cc0809cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate results\n",
    "total_indv = []\n",
    "for seed in nearest_words_indv:\n",
    "    for word in nearest_words_indv[seed]:\n",
    "        total_indv.append(word)\n",
    "\n",
    "total_coll = []\n",
    "for seed in nearest_words_coll:\n",
    "    for word in nearest_words_coll[seed]:\n",
    "        total_coll.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b1c3608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD_SINGLE = 0.75\n",
    "THRESHOLD_CLUSTER = 0.45\n",
    "\n",
    "final_indv = []\n",
    "#Add seed words to final lexica with distance 1\n",
    "for w in seed_words_indv:\n",
    "    final_indv.append((w, 1))\n",
    "\n",
    "#add words from single word cluster expansion that also overlap with joint cluster expansion\n",
    "#where distance from expanded word --> original word is > THRESHOLD_SINGLE\n",
    "final_temp_keys = list(zip(*final_indv))[0]\n",
    "lexica_indv_keys = list(zip(*lexica_indv))[0]\n",
    "for w1 in total_indv:\n",
    "    if (w1[0] in lexica_indv_keys and w1[1] > THRESHOLD_SINGLE and w1[0] not in final_temp_keys):\n",
    "        final_indv.append(w1)\n",
    "\n",
    "#add words from joint cluster expansion \n",
    "#where distance from expanded word --> original centroid is > THRESHOLD_CLUSTER\n",
    "final_temp_keys = list(zip(*final_indv))[0]\n",
    "for w in lexica_indv:\n",
    "    if(w[1] > THRESHOLD_CLUSTER and w[0] not in final_temp_keys):\n",
    "        final_indv.append(w)\n",
    "\n",
    "final_coll = []\n",
    "#Add seed words to final lexica with distance 1\n",
    "for w in seed_words_coll:\n",
    "    final_coll.append((w, 1))\n",
    "\n",
    "#add words from single word cluster expansion that also overlap with joint cluster expansion\n",
    "#where distance from expanded word --> original word is > THRESHOLD_SINGLE\n",
    "final_temp_keys = list(zip(*final_coll))[0]\n",
    "lexica_coll_keys = list(zip(*lexica_coll))[0]\n",
    "for w1 in total_coll:\n",
    "    if (w1[0] in lexica_coll_keys and w1[1] > THRESHOLD_SINGLE and w1[0] not in final_temp_keys):\n",
    "        final_coll.append(w1)\n",
    "\n",
    "#add words from joint cluster expansion \n",
    "#where distance from expanded word --> original centroid is > THRESHOLD_CLUSTER\n",
    "final_temp_keys = list(zip(*final_coll))[0]\n",
    "for w in lexica_coll:\n",
    "    if(w[1] > THRESHOLD_CLUSTER and w[0] not in final_temp_keys):\n",
    "        final_coll.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "15751241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2519370"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove words with tokenizing errors\n",
    "THRESHOLD_WIKI = 50\n",
    "THRESHOLD_WORDFREQ = 1e-4\n",
    "\n",
    "final_indv_parsed = parse_list(final_indv, THRESHOLD_WIKI, THRESHOLD_WORDFREQ)\n",
    "final_coll_parsed = parse_list(final_coll, THRESHOLD_WIKI, THRESHOLD_WORDFREQ)\n",
    "\n",
    "len(wiki_en_words)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "lmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Oct 18 2022, 18:57:03) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ecdbd8d05ba495d4a41ea56b380eb3c348934e8aa7cd13271a79d86ff4355567"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
